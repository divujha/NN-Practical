{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FUZZY LOGIC"
      ],
      "metadata": {
        "id": "93GqouRpPoZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yIA3GtC97LE",
        "outputId": "41779988-da8f-4708-d4a6-1e923a200a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temp value23.5\n",
            "press value1.7\n",
            "Rule One\n",
            "Temp is BA\n",
            "value of zt: 0.5666666666666667\n",
            "Pressure is BA\n",
            "value of zp: 0.6\n",
            "Rule Two\n",
            "Temp is low\n",
            "value of zt1: 0.1\n",
            "Pressure is low\n",
            "value of zp1: 0.06666666666666672\n",
            "value of z1: 0.57\n",
            "value of z2: 0.07\n",
            "Rule One for HP\n",
            "HP is MH\n",
            "vhp1: 0.75\n",
            "Rule Two for HP\n",
            "HP is High\n",
            "vhp2: 0.75\n",
            "crip of HP: 4.109375\n",
            "Rule One for VO\n",
            "VO is ML\n",
            "vvo1: 0.75\n",
            "Rule Two for VO\n",
            "VO is small\n",
            "vvo2: 0.75\n",
            "crip of VO: 1.890625\n"
          ]
        }
      ],
      "source": [
        "x=float(input(\"temp value\"))\n",
        "y=float(input(\"press value\"))\n",
        "print(\"Rule One\")\n",
        "print(\"Temp is BA\")\n",
        "r=45\n",
        "l=15\n",
        "c=(l+r)/2\n",
        "if x>c and x<=r:\n",
        "  zt=(r-x)/(r-c)\n",
        "elif x<=c and x>=l:\n",
        "  zt=(x-l)/(c-l)\n",
        "else:\n",
        "  zt=0\n",
        "print(\"value of zt:\",zt)\n",
        "\n",
        "print(\"Pressure is BA\")\n",
        "r=2.75\n",
        "l=1.25\n",
        "c=(l+r)/2\n",
        "if y>c and y<=r:\n",
        "  zp=(r-y)/(r-c)\n",
        "elif y<=c and y>=l:\n",
        "  zp=(y-l)/(c-l)\n",
        "else:\n",
        "  zp=0\n",
        "print(\"value of zp:\",zp)\n",
        "\n",
        "print(\"Rule Two\")\n",
        "print(\"Temp is low\")\n",
        "r=25\n",
        "c=10\n",
        "d=r-c\n",
        "l=c-d\n",
        "if x>c and x<=r:\n",
        "  zt1=(r-x)/(r-c)\n",
        "elif x<=c and x>=l:\n",
        "  zt1=(x-l)/(c-l)\n",
        "else:\n",
        "  zt1=0\n",
        "print(\"value of zt1:\",zt1)\n",
        "\n",
        "print(\"Pressure is low\")\n",
        "r=1.75\n",
        "c=1\n",
        "d=r-c\n",
        "l=c-d\n",
        "if y>c and y<=r:\n",
        "  zp1=(r-y)/(r-c)\n",
        "elif y<=c and y>=l:\n",
        "  zp1=(y-l)/(c-l)\n",
        "else:\n",
        "  zp1=0\n",
        "print(\"value of zp1:\",zp1)\n",
        "\n",
        "z1=min(zt,zp)\n",
        "z2=min(zt1,zp1)\n",
        "round_z1=round(z1,2)\n",
        "print(\"value of z1:\",round_z1)\n",
        "round_z2=round(z2,2)\n",
        "print(\"value of z2:\",round_z2)\n",
        "\n",
        "print(\"Rule One for HP\")\n",
        "print(\"HP is MH\")\n",
        "lhp1=3.25\n",
        "rhp1=4.75\n",
        "chp1=(lhp1+rhp1)/2\n",
        "bhp1=rhp1-lhp1\n",
        "vhp1=(1/2)*(bhp1)*(1)\n",
        "print(\"vhp1:\",vhp1)\n",
        "\n",
        "print(\"Rule Two for HP\")\n",
        "print(\"HP is High\")\n",
        "lhp2=4.25\n",
        "chp2=5\n",
        "dhp2=chp2-lhp2\n",
        "rhp2=chp2+dhp2\n",
        "bhp2=rhp2-lhp2\n",
        "vhp2=(1/2)*(bhp2)*(1)\n",
        "print(\"vhp2:\",vhp2)\n",
        "\n",
        "crip_HP=(((round_z1)*(vhp1)*(chp1))+((round_z2)*(vhp2)*(chp2)))/(((round_z1)*(vhp1))+((round_z2)*(vhp2)))\n",
        "print(\"crip of HP:\",crip_HP)\n",
        "\n",
        "print(\"Rule One for VO\")\n",
        "print(\"VO is ML\")\n",
        "lvo1=1.25\n",
        "rvo1=2.75\n",
        "cvo1=(lvo1+rvo1)/2\n",
        "bvo1=rvo1-lvo1\n",
        "vvo1=(1/2)*(bvo1)*(1)\n",
        "print(\"vvo1:\",vvo1)\n",
        "\n",
        "print(\"Rule Two for VO\")\n",
        "print(\"VO is small\")\n",
        "rvo2=1.75\n",
        "cvo2=1\n",
        "dvo2=rvo2-cvo2\n",
        "lvo2=cvo2-dvo2\n",
        "bvo2=rvo2-lvo2\n",
        "vvo2=(1/2)*(bvo2)*(1)\n",
        "print(\"vvo2:\",vvo2)\n",
        "\n",
        "crip_VO=(((round_z1)*(vvo1)*(cvo1))+((round_z2)*(vvo2)*(cvo2)))/(((round_z1)*(vvo1))+((round_z2)*(vvo2)))\n",
        "print(\"crip of VO:\",crip_VO)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BAM"
      ],
      "metadata": {
        "id": "pw9XoFXwPu3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "mem_x=np.array([[1,0,1,0],[1,1,0,1]])\n",
        "mem_y=np.array([[1,0,0,1],[0,1,0,1]])\n",
        "\n",
        "wt_mem=np.dot(mem_x.T,mem_y)\n",
        "\n",
        "probe=np.array([1,0,0,1])\n",
        "signal_x=2*probe-1\n",
        "signal_y=np.random.choice([-1,1],mem_y.shape[1])\n",
        "\n",
        "pattern_x=[signal_x]\n",
        "pattern_y=[signal_y]\n",
        "\n",
        "while True:\n",
        "  signal_x=np.sign(np.dot(signal_y,wt_mem))\n",
        "  pattern_x.append(signal_x)\n",
        "\n",
        "  signal_y=np.sign(np.dot(signal_x,wt_mem))\n",
        "  pattern_y.append(signal_y)\n",
        "\n",
        "  if np.array_equal(signal_x,pattern_x[-2]) and np.array_equal(signal_y,pattern_y[-2]):\n",
        "    break\n",
        "\n",
        "print(\"Updated trace of FX:\", pattern_x)\n",
        "print(\"Updated trace of FX:\", pattern_y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwTFeCzdMFJy",
        "outputId": "81153754-df6f-46ca-9726-9e135743a030"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated trace of FX: [array([ 1, -1, -1,  1]), array([1, 1, 0, 1]), array([1, 1, 0, 1])]\n",
            "Updated trace of FX: [array([ 1, -1,  1,  1]), array([1, 1, 0, 1]), array([1, 1, 0, 1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BAM"
      ],
      "metadata": {
        "id": "0_N9G91f2bh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "mem_vectors=np.array([\n",
        "\n",
        "    [1,1,0,0,0,0],\n",
        "\n",
        "    [0,0,0,0,1,1],\n",
        "\n",
        "    [0,0,1,1,0,0]\n",
        "\n",
        "])\n",
        "\n",
        "q=mem_vectors.shape[0] #number of vectors\n",
        "\n",
        "n=mem_vectors.shape[1] #dim of the vectors\n",
        "\n",
        "bip_mem_vecs=2*mem_vectors-1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Initialize and compute the weight matrix\n",
        "\n",
        "zd_wt_mat=np.zeros((n,n))\n",
        "\n",
        "for i in range (q):\n",
        "\n",
        "  zd_wt_mat +=np.outer(bip_mem_vecs[i], bip_mem_vecs[i])\n",
        "\n",
        "zd_wt_mat -=q*np.eye(n) #zero diag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "probe=input(\"Enter the Probe vector\")\n",
        "\n",
        "probe=np.array([int(x) for x in probe.split()])\n",
        "\n",
        "signal_vector=2*probe-1\n",
        "\n",
        "print(signal_vector)\n",
        "\n",
        "\n",
        "\n",
        "flag = 0 #Intialize flag\n",
        "\n",
        "while flag !=n:\n",
        "\n",
        "  permindex=np.random.permutation(n) #Randomize order\n",
        "\n",
        "  old_signal_vector= signal_vector.copy()\n",
        "\n",
        "  for j in range(n):\n",
        "\n",
        "    act_vec=np.dot(signal_vector,zd_wt_mat)\n",
        "\n",
        "    if act_vec[permindex[j]]>0:\n",
        "\n",
        "      signal_vector[permindex[j]]=1\n",
        "\n",
        "    elif act_vec[permindex[j]]<0:\n",
        "\n",
        "      signal_vector[permindex[j]]=-1\n",
        "\n",
        "  flag = np.dot(signal_vector,old_signal_vector)\n",
        "\n",
        "\n",
        "\n",
        "print(\"The recalled vector is :\")\n",
        "\n",
        "print(0.5*(signal_vector + 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1_NL72epzMg",
        "outputId": "6a0c84c0-44ef-41e5-9ee2-aa3cfbc86a22"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the Probe vector0 0 0 0 1 1\n",
            "[-1 -1 -1 -1  1  1]\n",
            "The recalled vector is :\n",
            "[0. 0. 0. 0. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RBF"
      ],
      "metadata": {
        "id": "LjkE4hVj2n1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data from CSV\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/heart.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess the data if needed (e.g., encoding categorical variables)\n",
        "\n",
        "# Assuming no preprocessing needed in this case\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split data into input features (X) and target variable (Y)\n",
        "\n",
        "X = data.drop(\"output\", axis=1)\n",
        "\n",
        "Y = data[\"output\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Standardize input features\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Perform K-Means clustering\n",
        "\n",
        "K_cent = 8\n",
        "\n",
        "km = KMeans(n_clusters=K_cent, max_iter=100)\n",
        "\n",
        "km.fit(X_scaled)\n",
        "\n",
        "cent = km.cluster_centers_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate sigma\n",
        "\n",
        "max_dist = 0\n",
        "\n",
        "for i in range(K_cent):\n",
        "\n",
        "    for j in range(K_cent):\n",
        "\n",
        "        dist = np.linalg.norm(cent[i] - cent[j])\n",
        "\n",
        "        if dist > max_dist:\n",
        "\n",
        "            max_dist = dist\n",
        "\n",
        "sigma = max_dist / math.sqrt(2 * K_cent)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute RBF transformation matrix for training data\n",
        "\n",
        "row, column = X_scaled.shape\n",
        "\n",
        "G = np.empty((row, K_cent), dtype=float)\n",
        "\n",
        "for i in range(row):\n",
        "\n",
        "    for j in range(K_cent):\n",
        "\n",
        "        dist = np.linalg.norm(X_scaled[i] - cent[j])\n",
        "\n",
        "        G[i][j] = math.exp(-math.pow(dist, 2) / math.pow(2 * sigma, 2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute weights\n",
        "\n",
        "GTG = np.dot(G.T, G)\n",
        "\n",
        "GTG_inv = np.linalg.inv(GTG)\n",
        "\n",
        "fac = np.dot(GTG_inv, G.T)\n",
        "\n",
        "W = np.dot(fac, Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute RBF transformation matrix for testing data\n",
        "\n",
        "row_test, _ = X_test.shape\n",
        "\n",
        "G_test = np.empty((row_test, K_cent), dtype=float)\n",
        "\n",
        "for i in range(row_test):\n",
        "\n",
        "    for j in range(K_cent):\n",
        "\n",
        "        dist = np.linalg.norm(X_test[i] - cent[j])\n",
        "\n",
        "        G_test[i][j] = math.exp(-math.pow(dist, 2) / math.pow(2 * sigma, 2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Predict output\n",
        "\n",
        "prediction = np.dot(G_test, W)\n",
        "\n",
        "prediction = 0.5 * (np.sign(prediction - 0.5) + 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "\n",
        "score = accuracy_score(prediction, Y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)\n"
      ],
      "metadata": {
        "id": "_bt5wPV22dy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SLP"
      ],
      "metadata": {
        "id": "K364yyEU3bVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make a prediction with weight\n",
        "\n",
        "def predict(row,weights):\n",
        "\n",
        "    activation=weights[0] #w0 taken by bias\n",
        "\n",
        "    for i in range (len(row)-1): #increament\n",
        "\n",
        "        activation+=weights[i+1]*row[i] #since w0 is taken by bias we will start from w1\n",
        "\n",
        "    return 1.0 if activation>=0.0 else 0.0 #activation function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\n",
        "[1.465489372,2.362125076,0],\n",
        "\n",
        "[3.396561688,4.400293529,0],\n",
        "\n",
        "[1.38807019,1.850220317,0],\n",
        "\n",
        "[3.06407232,3.005305973,0],\n",
        "\n",
        "[7.627531214,2.759262235,1],\n",
        "\n",
        "[5.332441248,2.088626775,1],\n",
        "\n",
        "[6.922596716,1.77106367,1],\n",
        "\n",
        "[8.675418651,-0.242068655,1],\n",
        "\n",
        "[7.673756466,3.508563011,1]]\n",
        "\n",
        "\n",
        "\n",
        "weights = [-0.1, 0.20653640140000007, -0.23418117710000003]\n",
        "\n",
        "\n",
        "\n",
        "for row in dataset:\n",
        "\n",
        "    prediction=predict(row,weights)\n",
        "\n",
        "    print(\"Excepted=%d,Prediction=%d\" % (row[-1],prediction)) #it print the last row values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Estimate Perceptron weight using stochastic gradient descent\n",
        "\n",
        "def train_weights(train,l_rate,n_epoch):\n",
        "\n",
        "    weights=[0.0 for i in range(len(train[0]))]\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "\n",
        "        sum_error=0.0\n",
        "\n",
        "        for row in train:\n",
        "\n",
        "            prediction=predict(row,weights)\n",
        "\n",
        "            error=row[-1]-prediction #expected-predicted=error\n",
        "\n",
        "            sum_error+=error**2\n",
        "\n",
        "            weights[0]=weights[0]+l_rate*error\n",
        "\n",
        "            for i in range(len(row)-1):\n",
        "\n",
        "                weights[i+1]=weights[i+1]+l_rate*error*row[i]\n",
        "\n",
        "        print('>epoch=%d,lrate=%.3f,error=%.3f' % (epoch,l_rate,sum_error))\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "\n",
        "l_rate=0.1\n",
        "\n",
        "n_epoch=5\n",
        "\n",
        "weights=train_weights(dataset,l_rate,n_epoch)\n",
        "\n",
        "print(weights)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aKsqyJH3UEr",
        "outputId": "66abf800-cb2c-4de4-a3c3-35f7e5f11c2b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excepted=0,Prediction=0\n",
            "Excepted=0,Prediction=0\n",
            "Excepted=0,Prediction=0\n",
            "Excepted=0,Prediction=0\n",
            "Excepted=0,Prediction=0\n",
            "Excepted=1,Prediction=1\n",
            "Excepted=1,Prediction=1\n",
            "Excepted=1,Prediction=1\n",
            "Excepted=1,Prediction=1\n",
            "Excepted=1,Prediction=1\n",
            ">epoch=0,lrate=0.100,error=2.000\n",
            ">epoch=1,lrate=0.100,error=1.000\n",
            ">epoch=2,lrate=0.100,error=0.000\n",
            ">epoch=3,lrate=0.100,error=0.000\n",
            ">epoch=4,lrate=0.100,error=0.000\n",
            "[-0.1, 0.20653640140000007, -0.23418117710000003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP WITH KERAS"
      ],
      "metadata": {
        "id": "O9E4itCM34GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dependencies\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "path =\"drive/My Drive/Data/Bitcoin Trust/\"\n",
        "\n",
        "dataset = pd.read_csv(path+'NNInput.csv', header = None)\n",
        "\n",
        "\n",
        "\n",
        "#path =\"drive/My Drive/Data/Digg/\"\n",
        "\n",
        "#dataset = pd.read_csv(path+'NNInput.txt',sep = '\\t', header = None)\n",
        "\n",
        "#dataset = dataset.astype(int)\n",
        "\n",
        "\n",
        "\n",
        "dataset.head()\n",
        "\n",
        "\n",
        "\n",
        "input_cols = len(dataset.columns) - 1\n",
        "\n",
        "output_col = len(dataset.columns)\n",
        "\n",
        "output_classes = len(set(dataset.iloc[:,-1]))\n",
        "\n",
        "\n",
        "\n",
        "X = dataset.iloc[:,:input_cols].values\n",
        "\n",
        "y = dataset.iloc[:,input_cols:output_col].values\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "y = ohe.fit_transform(y).toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
        "\n",
        "\n",
        "\n",
        "#from sklearn.decomposition import PCA\n",
        "\n",
        "#from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pca = PCA(n_components = 500)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#X_train = pca.fit_transform(X_train)\n",
        "\n",
        "#X_test = pca.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "\n",
        "\n",
        "input_cols = X_train.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "#!pip install absl-py\n",
        "\n",
        "\n",
        "\n",
        "#Dependencies\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "#from keras.layers.core import Dropout\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "#Dropout(0.25),\n",
        "\n",
        "#num_Rows * AbsolongE / input_cols\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(16, input_dim=input_cols , activation='relu',  kernel_regularizer=regularizers.l2(0.1)))\n",
        "\n",
        "model.add(Dense(12, activation='relu',  kernel_regularizer=regularizers.l2(0.1)))\n",
        "\n",
        "model.add(Dense(output_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "\n",
        "                                   mode='min',\n",
        "\n",
        "                                   patience=10,\n",
        "\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "istory = model.fit(X_train, y_train, epochs=50,  shuffle=True, validation_split=0.2)\n",
        "\n",
        "\n",
        "\n",
        "#y_pred = ridgereg.predict(X_test)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Converting predictions to label\n",
        "\n",
        "pred = list()\n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "\n",
        "#Converting one hot encoded test label to label\n",
        "\n",
        "test = list()\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "\n",
        "    test.append(np.argmax(y_test[i]))\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a = accuracy_score(pred,test)\n",
        "\n",
        "print('Accuracy is:', a)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model Accuracy')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "matrix = confusion_matrix(test, pred)\n",
        "\n",
        "matrix\n",
        "\n",
        "\n",
        "\n",
        "print(classification_report(test, pred))\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "\n",
        "lr_roc_auc_multiclass = roc_auc_score_multiclass(test, pred)\n",
        "\n",
        "print(lr_roc_auc_multiclass)"
      ],
      "metadata": {
        "id": "Zb_lTHk03qUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP"
      ],
      "metadata": {
        "id": "Ag8zZ2w44kHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize a network\n",
        "\n",
        "from random import random\n",
        "\n",
        "from random import seed\n",
        "\n",
        "\n",
        "\n",
        "def initialize_network(n_inputs,n_hidden,n_outputs):\n",
        "\n",
        "    network=list()\n",
        "\n",
        "    hidden_layer = [{'weights':[random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
        "\n",
        "    network.append(hidden_layer)\n",
        "\n",
        "    output_layer= [{'weights':[random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
        "\n",
        "    network.append(output_layer)\n",
        "\n",
        "    return network\n",
        "\n",
        "\n",
        "\n",
        "seed(1)\n",
        "\n",
        "network=initialize_network(2,1,2)\n",
        "\n",
        "for layer in network:\n",
        "\n",
        "    print(layer)\n",
        "\n",
        "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
        "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#calculate neuron activation for an input\n",
        "\n",
        "def activate(weights,inputs):\n",
        "\n",
        "    activate=weights[-1]\n",
        "\n",
        "    for i in range (len(weights)-1):\n",
        "\n",
        "        activate+=weights[i]*inputs[i]\n",
        "\n",
        "        return activate\n",
        "\n",
        "\n",
        "\n",
        "#Transfer neuron activation\n",
        "\n",
        "from math import exp\n",
        "\n",
        "def transfer(activation):\n",
        "\n",
        "    return 1.0/(1.0+exp(-activation))\n",
        "\n",
        "\n",
        "\n",
        "#Forward propogate input to a network output\n",
        "\n",
        "def forward_propogate(network,row):\n",
        "\n",
        "    inputs=row\n",
        "\n",
        "    for layer in network:\n",
        "\n",
        "        new_inputs=[]\n",
        "\n",
        "        for neuron in layer:\n",
        "\n",
        "            activation=activate(neuron['weights'],inputs)\n",
        "\n",
        "            neuron['output']=transfer(activation)\n",
        "\n",
        "            new_inputs.append(neuron['output'])\n",
        "\n",
        "            inputs=new_inputs\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "\n",
        "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\n",
        "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "\n",
        "row=[1,0,None]\n",
        "\n",
        "output=forward_propogate(network,row)\n",
        "\n",
        "print(output)\n",
        "\n",
        "\n",
        "\n",
        "[0.6629970129852887, 0.7210355736949032]\n",
        "\n",
        "\n",
        "\n",
        "#Calculate the derivative of an neuron output\n",
        "\n",
        "def transfer_derivative(output):\n",
        "\n",
        "    return output*(1.0-output)\n",
        "\n",
        "\n",
        "\n",
        "#Backpropagate error and store in neurons\n",
        "\n",
        "def backward_propagate_error(network,expected):\n",
        "\n",
        "    for i in reversed(range(len(network))):\n",
        "\n",
        "        layer=network[i]\n",
        "\n",
        "        errors=list()\n",
        "\n",
        "        if i != len(network)-1:\n",
        "\n",
        "            for j in range(len(layer)):\n",
        "\n",
        "                error=0.0\n",
        "\n",
        "                for neuron in network[i+1]:\n",
        "\n",
        "                    error+=(neuron['weights'][j]*neuron['delta'])\n",
        "\n",
        "                    errors.append(error)\n",
        "\n",
        "        else:\n",
        "\n",
        "            for j in range(len(layer)):\n",
        "\n",
        "                neuron=layer[j]\n",
        "\n",
        "                errors.append(neuron['output']-expected[j])\n",
        "\n",
        "        for j in range(len(layer)):\n",
        "\n",
        "            neuron=layer[j]\n",
        "\n",
        "            neuron['delta']=errors[j]*transfer_derivative(neuron['output'])\n",
        "\n",
        "\n",
        "\n",
        "# test backpropagation of error\n",
        "\n",
        "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\n",
        "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "\n",
        "expected = [0, 1]\n",
        "\n",
        "backward_propagate_error(network, expected)\n",
        "\n",
        "for layer in network:\n",
        "\n",
        "print(layer)\n",
        "\n",
        "\n",
        "\n",
        "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': 0.007668854370284511}]\n",
        "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': 0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': -0.0771723774346327}]\n",
        "\n",
        "\n",
        "\n",
        "#Bias is free parameter it can be change , every neuron has only one bias therefore we not gonna change the bias by using for loop we have to update only once\n",
        "\n",
        "\n",
        "#Update network weights with error\n",
        "\n",
        "def update_weights(network,row,l_rate):\n",
        "\n",
        "    for i in range(len(network)):\n",
        "\n",
        "        inputs=row[:-1]\n",
        "\n",
        "        if i !=0:\n",
        "\n",
        "            inputs=[neuron['output'] for neuron in network[i-1]]\n",
        "\n",
        "            for neuron in network[i]:\n",
        "\n",
        "                for j in range(len(inputs)):\n",
        "\n",
        "                    neuron['weights'][j]-=l_rate*neuron['delta']*inputs[j]\n",
        "\n",
        "                neuron['weights'][-1]-=l_rate*neuron['delta']\n",
        "\n",
        "\n",
        "\n",
        "# Train a network for a fixed number of epochs\n",
        "\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "\n",
        "        sum_error = 0\n",
        "\n",
        "        for row in train:\n",
        "\n",
        "            outputs = forward_propogate(network, row)\n",
        "\n",
        "            expected = [0 for i in range(n_outputs)]\n",
        "\n",
        "            expected[row[-1]] = 1\n",
        "\n",
        "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "\n",
        "            backward_propagate_error(network, expected)\n",
        "\n",
        "            update_weights(network, row, l_rate)\n",
        "\n",
        "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "\n",
        "\n",
        "\n",
        "# Test training backprop algorithm\n",
        "\n",
        "seed(1)\n",
        "\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\n",
        "     [1.465489372,2.362125076,0],\n",
        "\n",
        "     [3.396561688,4.400293529,0],\n",
        "\n",
        "     [1.38807019,1.850220317,0],\n",
        "\n",
        "     [3.06407232,3.005305973,0],\n",
        "\n",
        "     [7.627531214,2.759262235,1],\n",
        "\n",
        "     [5.332441248,2.088626775,1],\n",
        "\n",
        "     [6.922596716,1.77106367,1],\n",
        "\n",
        "     [8.675418651,-0.242068655,1],\n",
        "\n",
        "     [7.673756466,3.508563011,1]]\n",
        "\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "\n",
        "train_network(network, dataset, 0.5, 20, n_outputs)\n",
        "\n",
        "for layer in network:\n",
        "\n",
        "    print(layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#XOR DATASET\n",
        "\n",
        "\n",
        "\n",
        "from random import random\n",
        "\n",
        "from random import seed\n",
        "\n",
        "from math import exp\n",
        "\n",
        "\n",
        "\n",
        "#Initialize the network\n",
        "\n",
        "def initialize_network(n_inputs,n_hidden,n_outputs):\n",
        "\n",
        "    network = list()\n",
        "\n",
        "    hidden_layer = [{'weights': [random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
        "\n",
        "    network.append(hidden_layer)\n",
        "\n",
        "    output_layer = [{'weights': [random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
        "\n",
        "    network.append(output_layer)\n",
        "\n",
        "    return network\n",
        "\n",
        "# Calculate neuron activation for an input\n",
        "\n",
        "def activate(weights, inputs):\n",
        "\n",
        "    activation = weights[-1]\n",
        "\n",
        "    for i in range(len(weights) - 1):\n",
        "\n",
        "        activation += weights[i] * inputs[i]\n",
        "\n",
        "    return activation\n",
        "\n",
        "# Transfer neuron activation\n",
        "\n",
        "def transfer(activation):\n",
        "\n",
        "    return 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "# Forward propagate input to a network output\n",
        "\n",
        "def forward_propagate(network, row):\n",
        "\n",
        "    inputs = row\n",
        "\n",
        "    for layer in network:\n",
        "\n",
        "        new_inputs = []\n",
        "\n",
        "        for neuron in layer:\n",
        "\n",
        "            activation = activate(neuron['weights'], inputs)\n",
        "\n",
        "            neuron['output'] = transfer(activation)\n",
        "\n",
        "            new_inputs.append(neuron['output'])\n",
        "\n",
        "        inputs = new_inputs\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Calculate the derivative of an neuron output\n",
        "\n",
        "def transfer_derivative(output):\n",
        "\n",
        "    return output * (1.0 - output)\n",
        "\n",
        "# Backpropagate error and store in neurons\n",
        "\n",
        "def backward_propagate_error(network, expected):\n",
        "\n",
        "    for i in reversed(range(len(network))):\n",
        "\n",
        "        layer = network[i]\n",
        "\n",
        "        errors = list()\n",
        "\n",
        "        if i != len(network) - 1:\n",
        "\n",
        "            for j in range(len(layer)):\n",
        "\n",
        "                error = 0.0\n",
        "\n",
        "                for neuron in network[i + 1]:\n",
        "\n",
        "                    error += (neuron['weights'][j] * neuron['delta'])\n",
        "\n",
        "                errors.append(error)\n",
        "\n",
        "        else:\n",
        "\n",
        "            for j in range(len(layer)):\n",
        "\n",
        "                neuron = layer[j]\n",
        "\n",
        "                errors.append(neuron['output'] - expected[j])\n",
        "\n",
        "        for j in range(len(layer)):\n",
        "\n",
        "            neuron = layer[j]\n",
        "\n",
        "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        "\n",
        "# Update network weights with error\n",
        "\n",
        "def update_weights(network, row, l_rate):\n",
        "\n",
        "    for i in range(len(network)):\n",
        "\n",
        "        inputs = row[:-1]\n",
        "\n",
        "        if i != 0:\n",
        "\n",
        "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\n",
        "        for neuron in network[i]:\n",
        "\n",
        "            for j in range(len(inputs)):\n",
        "\n",
        "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
        "\n",
        "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
        "\n",
        "# Train the network for a fixed number of epochs\n",
        "\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "\n",
        "        sum_error = 0\n",
        "\n",
        "        for row in train:\n",
        "\n",
        "            outputs = forward_propagate(network, row)\n",
        "\n",
        "            expected = [0 for i in range(n_outputs)]\n",
        "\n",
        "            expected[row[-1]] = 1\n",
        "\n",
        "            sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in range(len(expected))])\n",
        "\n",
        "            backward_propagate_error(network, expected)\n",
        "\n",
        "            update_weights(network, row, l_rate)\n",
        "\n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "\n",
        "\n",
        "\n",
        "# Test training backprop algorithm\n",
        "\n",
        "seed(1)\n",
        "\n",
        "dataset = [[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]]\n",
        "\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "\n",
        "train_network(network, dataset, 0.5, 30000, n_outputs)\n",
        "\n",
        "output_string = \"\"\n",
        "\n",
        "for layer in network:\n",
        "\n",
        "    for neuron in layer:\n",
        "\n",
        "        output_string += str(neuron) + \"\\n\"\n",
        "\n",
        "print(output_string)\n",
        "\n",
        ">epoch=29999, lrate=0.500, error=0.001\n",
        "{'weights': [-8.435525364557988, -8.455155310451, 3.6756335756897935], 'output': 1.8229601371792782e-06, 'delta': -4.553817289345515e-09}\n",
        "{'weights': [-6.126821896937835, -6.128495402521643, 9.148043793202339], 'output': 0.042814484653166354, 'delta': 0.0001018363093315242}\n",
        "{'weights': [10.233752832088506, -10.180070478031773, 4.9235467726438955], 'output': 0.9888780422949028, 'delta': -0.00012232217990155932}\n",
        "{'weights': [-10.239512273884726, 10.185828800725224, -4.926355105815236], 'output': 0.011093814198379277, 'delta': 0.00012170736765205549}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Make a prediction with a network\n",
        "\n",
        "def predict(network, row):\n",
        "\n",
        "    outputs = forward_propagate(network, row)\n",
        "\n",
        "    return outputs.index(max(outputs))\n",
        "\n",
        "\n",
        "\n",
        "# Test making predictions with the network\n",
        "\n",
        "dataset = [[0,0,0],[0,1,1],[1,0,1],[1,1,0]]\n",
        "\n",
        "network = [[{'weights': [-8.435525364557973, -8.455155310450994, 3.6756335756897873]},\n",
        "\n",
        "{'weights': [-6.126821896937839, -6.128495402521647, 9.148043793202346]}],\n",
        "\n",
        "[{'weights':[10.23375283208851, -10.180070478031775, 4.923546772643896]},\n",
        "\n",
        "  {'weights': [-10.239512273884726, 10.185828800725222, -4.926355105815237]}]]\n",
        "\n",
        "for row in dataset:\n",
        "\n",
        "    prediction = predict(network, row)\n",
        "\n",
        "    print('Input= (%d, %d) Expected=%d, Got=%d' % (row[0], row[1],row[-1], prediction))\n",
        "\n",
        "\n",
        "\n",
        "Input= (0, 0) Expected=0, Got=0\n",
        "Input= (0, 1) Expected=1, Got=1\n",
        "Input= (1, 0) Expected=1, Got=1\n",
        "Input= (1, 1) Expected=0, Got=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#WHEAT SEEDS DATASETS\n",
        "\n",
        "\n",
        "\n",
        "# Backprop on Wheat Seeds Dataset\n",
        "\n",
        "from random import seed\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "from random import random\n",
        "\n",
        "from csv import reader\n",
        "\n",
        "from math import exp\n",
        "\n",
        "\n",
        "\n",
        "from csv import reader\n",
        "\n",
        "\n",
        "\n",
        "def load_csv(filename):\n",
        "\n",
        "    dataset = list()\n",
        "\n",
        "\n",
        "\n",
        "    with open(filename, 'r') as file:\n",
        "\n",
        "        csv_reader = reader(file)\n",
        "\n",
        "        for row in csv_reader:\n",
        "\n",
        "            if not row:\n",
        "\n",
        "                continue\n",
        "\n",
        "            dataset.append(row)\n",
        "\n",
        "\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "# Convert string column to float\n",
        "\n",
        "def str_column_to_float(dataset, column):\n",
        "\n",
        "    for row in dataset:\n",
        "\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "\n",
        "def str_column_to_int(dataset, column):\n",
        "\n",
        "    class_values = [row[column] for row in dataset]\n",
        "\n",
        "    unique = set(class_values)\n",
        "\n",
        "    lookup = dict()\n",
        "\n",
        "    for i, value in enumerate(unique):\n",
        "\n",
        "        lookup[value] = i\n",
        "\n",
        "    for row in dataset:\n",
        "\n",
        "        row[column] = lookup[row[column]]\n",
        "\n",
        "    return lookup\n",
        "\n",
        "\n",
        "\n",
        "# Find the min and max values for each column\n",
        "\n",
        "def dataset_minmax(dataset):\n",
        "\n",
        "    minmax = list()\n",
        "\n",
        "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "\n",
        "    return stats\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "\n",
        "def normalize_dataset(dataset, minmax):\n",
        "\n",
        "    for row in dataset:\n",
        "\n",
        "        for i in range(len(row)-1):\n",
        "\n",
        "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        "\n",
        "\n",
        "\n",
        "# Split a dataset into k folds\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\n",
        "    dataset_split = list()\n",
        "\n",
        "    dataset_copy = list(dataset)\n",
        "\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "\n",
        "    for i in range(n_folds):\n",
        "\n",
        "        fold = list()\n",
        "\n",
        "        while len(fold) < fold_size:\n",
        "\n",
        "            index = randrange(len(dataset_copy))\n",
        "\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "\n",
        "        dataset_split.append(fold)\n",
        "\n",
        "    return dataset_split\n",
        "\n",
        "#Calculate accuracy percentage\n",
        "\n",
        "def accuracy_metric(actual, predicted):\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(len(actual)):\n",
        "\n",
        "        if actual[i] == predicted[i]:\n",
        "\n",
        "            correct += 1\n",
        "\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "\n",
        "    scores = list()\n",
        "\n",
        "    for fold in folds:\n",
        "\n",
        "        train_set = list(folds)\n",
        "\n",
        "        train_set.remove(fold)\n",
        "\n",
        "        train_set = sum(train_set, [])\n",
        "\n",
        "        test_set = list()\n",
        "\n",
        "        for row in fold:\n",
        "\n",
        "            row_copy = list(row)\n",
        "\n",
        "            test_set.append(row_copy)\n",
        "\n",
        "            row_copy[-1] = None\n",
        "\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "\n",
        "        actual = [row[-1] for row in fold]\n",
        "\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "\n",
        "        scores.append(accuracy)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "#Calculate neuron activation for an input\n",
        "\n",
        "def activate(weights, inputs):\n",
        "\n",
        "    activation = weights[-1]\n",
        "\n",
        "    for i in range(len(weights)-1):\n",
        "\n",
        "        activation += weights[i] * inputs[i]\n",
        "\n",
        "    return activation\n",
        "\n",
        "\n",
        "\n",
        "# Transfer neuron activation\n",
        "\n",
        "def transfer(activation):\n",
        "\n",
        "    return 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "\n",
        "\n",
        "# Forward propagate input to a network output\n",
        "\n",
        "def forward_propagate(network, row):\n",
        "\n",
        "    inputs = row\n",
        "\n",
        "    for layer in network:\n",
        "\n",
        "        new_inputs = []\n",
        "\n",
        "        for neuron in layer:\n",
        "\n",
        "            activation = activate(neuron['weights'], inputs)\n",
        "\n",
        "            neuron['output'] = transfer(activation)\n",
        "\n",
        "            new_inputs.append(neuron['output'])\n",
        "\n",
        "        inputs = new_inputs\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the derivative of an neuron output\n",
        "\n",
        "def transfer_derivative(output):\n",
        "\n",
        "    return output * (1.0 - output)\n",
        "\n",
        "\n",
        "\n",
        "# Backpropagate error and store in neurons\n",
        "\n",
        "def backward_propagate_error(network, expected):\n",
        "\n",
        "    for i in reversed(range(len(network))):\n",
        "\n",
        "        layer = network[i]\n",
        "\n",
        "        errors = list()\n",
        "\n",
        "        if i != len(network)-1:\n",
        "\n",
        "            for j in range(len(layer)):\n",
        "\n",
        "                error = 0.0\n",
        "\n",
        "                for neuron in network[i + 1]:\n",
        "\n",
        "                    error += (neuron['weights'][j] * neuron['delta'])\n",
        "\n",
        "                errors.append(error)\n",
        "\n",
        "        else:\n",
        "\n",
        "            for j in range(len(layer)):\n",
        "\n",
        "                neuron = layer[j]\n",
        "\n",
        "                errors.append(neuron['output'] - expected[j])\n",
        "\n",
        "        for j in range(len(layer)):\n",
        "\n",
        "            neuron = layer[j]\n",
        "\n",
        "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        "\n",
        "\n",
        "\n",
        "# Update network weights with error\n",
        "\n",
        "def update_weights(network, row, l_rate):\n",
        "\n",
        "    for i in range(len(network)):\n",
        "\n",
        "        inputs = row[:-1]\n",
        "\n",
        "        if i != 0:\n",
        "\n",
        "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\n",
        "        for neuron in network[i]:\n",
        "\n",
        "            for j in range(len(inputs)):\n",
        "\n",
        "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
        "\n",
        "            neuron['weights'][-1] -= l_rate * neuron['delta']\n",
        "\n",
        "\n",
        "\n",
        "# Train a network for a fixed number of epochs\n",
        "\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "\n",
        "        for row in train:\n",
        "\n",
        "            outputs = forward_propagate(network, row)\n",
        "\n",
        "            expected = [0 for i in range(n_outputs)]\n",
        "\n",
        "            expected[row[-1]] = 1\n",
        "\n",
        "            backward_propagate_error(network, expected)\n",
        "\n",
        "            update_weights(network, row, l_rate)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize a network\n",
        "\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\n",
        "    network = list()\n",
        "\n",
        "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\n",
        "    network.append(hidden_layer)\n",
        "\n",
        "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\n",
        "    network.append(output_layer)\n",
        "\n",
        "    return network\n",
        "\n",
        "\n",
        "\n",
        "# Make a prediction with a network\n",
        "\n",
        "def predict(network, row):\n",
        "\n",
        "    outputs = forward_propagate(network, row)\n",
        "\n",
        "    return outputs.index(max(outputs))\n",
        "\n",
        "\n",
        "\n",
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "\n",
        "    n_inputs = len(train[0]) - 1\n",
        "\n",
        "    n_outputs = len(set([row[-1] for row in train]))\n",
        "\n",
        "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "\n",
        "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "\n",
        "    predictions = list()\n",
        "\n",
        "    for row in test:\n",
        "\n",
        "        prediction = predict(network, row)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    return(predictions)\n",
        "\n",
        "\n",
        "\n",
        "# Test Backprop on Seeds dataset\n",
        "\n",
        "seed(1)\n",
        "\n",
        "# load and prepare data\n",
        "\n",
        "filename = 'wheat-seeds.csv'\n",
        "\n",
        "dataset = load_csv(filename)\n",
        "\n",
        "for i in range(len(dataset[0])-1):\n",
        "\n",
        "str_column_to_float(dataset, i)\n",
        "\n",
        "# convert class column to integers\n",
        "\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "\n",
        "# normalize input variables\n",
        "\n",
        "minmax = dataset_minmax(dataset)\n",
        "\n",
        "normalize_dataset(dataset, minmax)\n",
        "\n",
        "# evaluate algorithm\n",
        "\n",
        "n_folds = 5\n",
        "\n",
        "l_rate = 0.3\n",
        "\n",
        "n_epoch = 500\n",
        "\n",
        "n_hidden = 5\n",
        "\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "\n",
        "\n",
        "\n",
        "Scores: [92.85714285714286, 92.85714285714286, 97.61904761904762, 95.23809523809523, 88.09523809523809]\n",
        "Mean Accuracy: 93.333%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "xr-UGWJV4eaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TuV99FiC4eJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}